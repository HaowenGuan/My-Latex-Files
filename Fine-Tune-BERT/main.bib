% TODO: Include DOIs (Digital Object Identifiers) (or as a second resort, the hyperlinked ACL An-thology Identifier) 
% Appropriate records should be found for most materials in the current ACL Anthology at http://aclanthology.info/.

@inproceedings{word2vec,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}

@inproceedings{glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@inproceedings{first-finetuned-pretraining,
  title={Semi-supervised sequence learning},
  author={Dai, Andrew M and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3079--3087},
  year={2015}
}

@inproceedings{gururangan-etal-2019-variational,
    title = "Variational Pretraining for Semi-supervised Text Classification",
    author = "Gururangan, Suchin  and
      Dang, Tam  and
      Card, Dallas  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
}

@article{openai-gpt,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={URL https://s3-us-west-2. amazonaws. com/openai-assets/research-covers/languageunsupervised/language understanding paper. pdf},
  year={2018}
}

@article{openai-gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={URL https://openai. com/blog/better-language-models},
  year={2019}
}

@article{openai-ulmfit,
  title={Universal language model fine-tuning for text classification},
  author={Howard, Jeremy and Ruder, Sebastian},
  journal={arXiv preprint arXiv:1801.06146},
  year={2018}
}

@inproceedings{nli_artifacts,
    title = "Annotation Artifacts in Natural Language Inference Data",
    author = "Gururangan, Suchin  and
      Swayamdipta, Swabha  and
      Levy, Omer  and
      Schwartz, Roy  and
      Bowman, Samuel  and
      Smith, Noah A.",
    booktitle = "NAACL",
    year = "2018"
}

@article{bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{xlnet,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V},
  journal={arXiv preprint arXiv:1906.08237},
  year={2019}
}

@misc{ernie,
    title={ERNIE 2.0: A Continual Pre-training Framework for Language Understanding},
    author={Yu Sun and Shuohuan Wang and Yukun Li and Shikun Feng and Hao Tian and Hua Wu and Haifeng Wang},
    year={2019},
    eprint={1907.12412},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{roberta,
    Author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
    Title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
    Year = {2019},
    Eprint = {arXiv:1907.11692},
}

@article{probing-bert,
  title={What do you learn from context? Probing for sentence structure in contextualized word representations},
  author={Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and Van Durme, Benjamin and Bowman, Samuel R and Das, Dipanjan and others},
  year={2018}
}

@inproceedings{Liang08structurecompilation,
    author = {Percy Liang and Hal Daum√© Iii and Dan Klein},
    title = {Structure compilation: trading structure for features},
    booktitle = {In Proceedings of ICML},
    year = {2008}
}

@inproceedings{model-compression,
  title={Model compression},
  author={Bucil{\u{a}}, C and Caruana, R and Niculescu-Mizil, A},
  booktitle={Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={535--541},
  year={2006}
}

@inproceedings{Gupta:2015,
 author = {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
 title = {Deep Learning with Limited Numerical Precision},
 booktitle = {ICML},
 year = {2015},
} 

@inproceedings{kim-etal-2017-adversarial,
    title = "Adversarial Adaptation of Synthetic or Stale Data",
    author = "Kim, Young-Bum  and
      Stratos, Karl  and
      Kim, Dongchan",
    booktitle = "ACL",
    year = "2017",
}

@inproceedings{kim-rush-2016-sequence,
    title = "Sequence-Level Knowledge Distillation",
    author = "Kim, Yoon  and
      Rush, Alexander M.",
    booktitle = "EMNLP",
    year = "2016",
}

@misc{tang2019distilling,
    title={Distilling Task-Specific Knowledge from BERT into Simple Neural Networks},
    author={Raphael Tang and Yao Lu and Linqing Liu and Lili Mou and Olga Vechtomova and Jimmy Lin},
    year={2019},
    eprint={1903.12136},
    archivePrefix={arXiv},
}

@incollection{is-depth-necessary,
    title = {Do Deep Nets Really Need to be Deep?},
    author = {Ba, Jimmy and Caruana, Rich},
    booktitle = {Advances in Neural Information Processing Systems 27},
    editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
    pages = {2654--2662},
    year = {2014},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep.pdf}
}

@article{distillation,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{distillation-dnn,
  title={Learning small-size DNN with output-distribution-based criteria},
  author={Li, Jinyu and Zhao, Rui and Huang, Jui-Ting and Gong, Yifan},
  booktitle={Fifteenth annual conference of the international speech communication association},
  year={2014}
}

@article{fitnets,
  title={Fitnets: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.6550},
  year={2014}
}

@inproceedings{yim2017gift,
  title={A gift from knowledge distillation: Fast optimization, network minimization and transfer learning},
  author={Yim, Junho and Joo, Donggyu and Bae, Jihoon and Kim, Junmo},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2017}
}

@article{distillation-quantization,
  title={Model compression via distillation and quantization},
  author={Polino, Antonio and Pascanu, Razvan and Alistarh, Dan},
  journal={arXiv preprint arXiv:1802.05668},
  year={2018}
}

@inproceedings{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  booktitle={NIPS},
  year={2016}
}

@article{like-what-you-like,
  title={Like what you like: Knowledge distill via neuron selectivity transfer},
  author={Huang, Zehao and Wang, Naiyan},
  journal={arXiv preprint arXiv:1707.01219},
  year={2017}
}

@inproceedings{dark-rank,
  title={Darkrank: Accelerating deep metric learning via cross sample similarities transfer},
  author={Chen, Yuntao and Wang, Naiyan and Zhang, Zhaoxiang},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{kimura2018few,
  title={Few-shot learning of neural networks from scratch by pseudo example optimization},
  author={Kimura, Akisato and Ghahramani, Zoubin and Takeuchi, Koh and Iwata, Tomoharu and Ueda, Naonori},
  journal={arXiv preprint arXiv:1802.03039},
  year={2018}
}

@article{born-again,
  title={Born again neural networks},
  author={Furlanello, Tommaso and Lipton, Zachary C and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1805.04770},
  year={2018}
}

@article{distillation-reading-comprehension,
  title={Attention-guided answer distillation for machine reading comprehension},
  author={Hu, Minghao and Peng, Yuxing and Wei, Furu and Huang, Zhen and Li, Dongsheng and Yang, Nan and Zhou, Ming},
  journal={arXiv preprint arXiv:1808.07644},
  year={2018}
}

@article{rmr,
  title={Reinforced mnemonic reader for machine reading comprehension},
  author={Hu, Minghao and Peng, Yuxing and Huang, Zhen and Qiu, Xipeng and Wei, Furu and Zhou, Ming},
  journal={arXiv preprint arXiv:1705.02798},
  year={2017}
}

@article{distillation-classification,
  title={Transformer to CNN: Label-scarce distillation for efficient text classification},
  author={Chia, Yew Ken and Witteveen, Sam and Andrews, Martin},
  year={2018}
}

@inproceedings{distillation-rnn-lm,
  title={Knowledge Distillation for Recurrent Neural Network Language Modeling with Trust Regularization},
  author={Shi, Yangyang and Hwang, Mei-Yuh and Lei, Xin and Sheng, Haoyu},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7230--7234},
  year={2019},
  organization={IEEE}
}

@article{truncated-pretraining,
  title={Model Compression with Multi-Task Knowledge Distillation for Web-scale Question Answering System},
  author={Yang, Ze and Shou, Linjun and Gong, Ming and Lin, Wutao and Jiang, Daxin},
  journal={arXiv preprint arXiv:1904.09636},
  year={2019}
}

@article{glue,
  title={Glue: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amapreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@INPROCEEDINGS{rte,
    author = {Luisa Bentivogli and Ido Dagan and Hoa Trang Dang and Danilo Giampiccolo and Bernardo Magnini},
    title = {The Fifth PASCAL Recognizing Textual Entailment Challenge},
    booktitle = {Proc Text Analysis Conference (TAC‚Äô09},
    year = {2009}
}

@incollection{LeCun:1998:CNI:303568.303704,
 author = {LeCun, Yann and Bengio, Yoshua},
 chapter = {Convolutional Networks for Images, Speech, and Time Series},
 title = {The Handbook of Brain Theory and Neural Networks},
 year = {1998},
 pages = {255--258},
 numpages = {4},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@inproceedings{johnson2015semi,
  title={Semi-supervised convolutional neural networks for text categorization via region embedding},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in neural information processing systems},
  pages={919--927},
  year={2015}
}

@article{hochreiter-schmidhuber:1997:_long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume=9,
  number=8,
  pages={1735--1780},
  year=1997,
  publisher={MIT Press}
}

@inproceedings{snli,
	Author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher, and Manning, Christopher D.},
	Booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	Publisher = {Association for Computational Linguistics},
	Title = {A large annotated corpus for learning natural language inference},
	Year = {2015}
}

@InProceedings{mnli,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of 
               the North American Chapter of the 
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}

@article{qqp,
  title={Quora Question Pairs},
  author={Chen, Z. and Zhang, H. and Zhang, X. and Zhao, L.},
  year={2018}
}

@inproceedings{sst2,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}

@inproceedings{amazon,
  title={Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering},
  author={He, Ruining and McAuley, Julian},
  booktitle={proceedings of the 25th international conference on world wide web},
  pages={507--517},
  year={2016},
  organization={International World Wide Web Conferences Steering Committee}
}

@inproceedings{transformer,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{elmo,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1802.05365},
  year={2018}
}

@article{wordpiece,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@inproceedings{skip-thought,
  title={Skip-thought vectors},
  author={Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan R and Zemel, Richard and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Advances in neural information processing systems},
  pages={3294--3302},
  year={2015}
}

@article{infer-sent,
  title={Supervised learning of universal sentence representations from natural language inference data},
  author={Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loic and Bordes, Antoine},
  journal={arXiv preprint arXiv:1705.02364},
  year={2017}
}

@inproceedings{spearman-original,
  title={The proof and measurement of association between two things.},
  author={Spearman},
  booktitle={American Journal of Psychology.},
  pages={72--101},
  year={1904}
}

@book{spearman-similarity,
  title={Frequency Analysis of English vocabulary and grammar, based on the LOB corpus},
  author={Johansson and Stig and Knut Hofland},
  year={1989},
  publisher={Claredon, Oxford}
}

@inproceedings{spearman-formula,
  title={Measures for corpus similarity and homogeneity},
  author={Kilgarriff, Adam and Rose, Tony},
  booktitle={Proceedings of the Third Conference on Empirical Methods for Natural Language Processing},
  pages={46--52},
  year={1998}
}

@inproceedings{books-dataset,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

@inproceedings{dissect_elmo,
  title={Dissecting contextual word embeddings: Architecture and representation},
  author={Peters, Matthew E and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
  booktitle={Proceedings of the 2018 conference on empirical methods in natural language processing},
  year={2018}
}

@inproceedings{clark-etal-2019-bam,
    title = "{BAM}! Born-Again Multi-Task Networks for Natural Language Understanding",
    author = "Clark, Kevin  and
      Luong, Minh-Thang  and
      Khandelwal, Urvashi  and
      Manning, Christopher D.  and
      Le, Quoc V.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
}

@article{patient_kd,
  title={Patient Knowledge Distillation for BERT Model Compression},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  journal={arXiv preprint arXiv:1908.09355},
  year={2019}
}

@article{jimmylin_kd,
  title={Distilling Task-Specific Knowledge from BERT into Simple Neural Networks},
  author={Tang, Raphael and Lu, Yao and Liu, Linqing and Mou, Lili and Vechtomova, Olga and Lin, Jimmy},
  journal={arXiv preprint arXiv:1903.12136},
  year={2019}
}

@article{xlm,
  title={Cross-lingual language model pretraining},
  author={Lample, Guillaume and Conneau, Alexis},
  journal={arXiv preprint arXiv:1901.07291},
  year={2019}
}

@article{distil_bert,
  title={Smaller, faster, cheaper, lighter: Introducing {DistilBERT}, a distilled version of {BERT}},
  author={Victor Sanh},
  journal={https://medium.com/huggingface/distilbert-8cf3380435b5},
  year={2019}
}