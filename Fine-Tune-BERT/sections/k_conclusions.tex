\section{Conclusion}
We conducted extensive experiments to gain understanding of how knowledge distillation and the \ptft algorithm work in isolation, and how they interact. We made the finding that their benefits compound, and unveiled the power of Pre-trained Distillation, a simple yet effective method to maximize the utilization of all available resources: a powerful teacher, and multiple sources of data (labeled sets, unlabeled transfer sets, and unlabeled LM sets).