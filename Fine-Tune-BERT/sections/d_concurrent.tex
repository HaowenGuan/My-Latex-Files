\section{Comparison to Concurrent Work}
\label{sec:concurrent}

\input{figures/0b_concurrent_work.tex}
There are concurrent efforts to ours aiming to leverage both pre-training and distillation in the context of building compact models. Though inspired by the two-stage \ptft approach that enabled deep-and-wide architectures to advance the state-of-the-art in language understanding, they depart from this traditional method in several key ways.

Patient Knowledge Distillation \citep{patient_kd} initializes a student from the bottom layers of a deeper pre-trained model, then performs task-specific \emph{patient} distillation. The training objective relies not only on the teacher output, but also on its intermediate layers, thus making assumptions about the student and teacher architectures. In a parallel line of work, DistilBert \citep{distil_bert} applies the same truncation-based initialization method for the student, then continues its LM pre-training via distillation from a more expensive LM teacher, and finally fine-tunes on task data. Its downside is that LM distillation is computationally expensive, as it requires a softmax operation over the entire vocabulary to compute the expensive LM teacher's predictive distribution. A common limitation in both studies is that the initialization strategy constrains the student to the teacher embedding size. Table \ref{tab:concurrent_desc} summarizes the differences between concurrent work and Pre-trained Distillation (PD). 

To facilitate direct comparison, in this section we perform an experiment with the same model architecture, sizes and dataset settings used in the two studies mentioned above. We perform Pre-trained Distillation on a 6-layer BERT student with task supervision from a 12-layer \bb teacher, using embedding size 768 for both models. For distillation, our transfer set coincides with the labeled set (\DT = \DL). Table \ref{tab:concurrent_results} reports results on the 6 GLUE tasks selected by \citet{patient_kd} and shows that, on average, PD performs best. For anchoring, we also provide quality numbers for \ptft (PF), which is surprisingly competitive to the more elaborate alternatives in this setting where  {\DT} is not larger than {\DL}. Remarkably, PF does not compromise generality or simplicity for quality. Its downside is, however, that it cannot leverage unlabeled task data and teacher model predictions.