\section{Problem Statement}
\label{sec:prelim}

Our high-level goal is to build accurate models which fit a given memory and latency budget. There are many aspects to explore: the parametric form of the compact model (architecture, number of parameters, trade-off between number of hidden layers and embedding size), the training data (size, distribution, presence or absence of labels, training objective), etc. Since an exhaustive search over this space is impractical, we fix the model architecture to bidirectional Transformers, known to be suitable for a wide range of NLP tasks \citep{transformer,bert}. The rest of this section elaborates on the training resources we assume to have at our disposal.

\textbf{The teacher} is a highly accurate but large model for an end task, that does not meet the resource constraints. Prior work on distillation often makes use of an ensemble of networks \citep{distillation}. For faster experimentation, we use a single teacher, without making a statement about the best architectural choice. In Section \ref{sec:concurrent}, the teacher is pre-trained \bb fine-tuned on labeled end-task data. In Section \ref{sec:exp}, we use \bertlarge instead. 

\textbf{Students} are compact models that satisfy resource constraints. Since model size qualifiers are relative (e.g., what is considered small in a data center can be impractically large on a mobile device), we investigate an array of 24 model sizes, from our \berttiny (4m parameters) all the way up to \bertbase (110m parameters)\footnote{Note that our \bertbase and BERT\textsubscript{BASE} in~\citet{bert} have the same architecture. We use the former term for clarity, since not all students in Section \ref{sec:exp} are pre-trained.}. The student model sizes and their relative speed-up compared to the \bertlarge teacher can be found in Table \ref{tab:bert-models}. Interested readers can situate themselves on this spectrum based on their resource constraints. For readability, most plots show a selection of 5 models, but we verify that our conclusions hold for all 24.

\textbf{Labeled data ($\D_{L}$)} is a set of $N$ training examples $\{(x_1, y_1), ..., (x_N, y_N)\}$, where $x_i$ is an input and $y_i$ is a label. For most NLP tasks, labeled sets are hard to produce and thus restricted in size.

%\input{figures/datasets_table.tex}
\textbf{Unlabeled transfer data ($\D_{T}$)} is a set of $M$ input examples of the form $\{x'_1, ..., x'_M\}$ sampled from a distribution that is similar to but possibly not identical to the input distribution of the labeled set. During distillation, the teacher \emph{transfers} knowledge to the student by exposing its label predictions for instances $x'_m$. $\D_{T}$ can also include the input portion of labeled data $\D_{L}$ instances. Due to the lack of true labels, such sets are generally easier to produce and consequently larger than labeled ones. Note, however, that task-relevant input text is not readily available for key tasks requiring paired texts such as natural language inference and question answering, as well as domain-specific dialog understanding. In addition, for deployed systems, input data distribution shifts over time  and existing unlabeled data becomes stale~\citep{kim-etal-2017-adversarial}.

\textbf{Unlabeled language model data ($\D_{LM}$)} is a collection of natural language texts that enable unsupervised learning of text representations. We use it for unsupervised pre-training with a masked language model objective~\citep{bert}. Because no labels are needed and strong domain similarity is not required, these corpora are often  vast, containing thousands of millions of words.

The distinction between the three types of datasets is strictly functional. Note they are not necessarily disjunct. For instance, the same corpus that forms the labeled data can also be part of the unlabeled transfer set, after its labels are discarded. Similarly, corpora that are included in the transfer set can also be used as unlabeled LM data.

\input{figures/size_and_latencies.tex}