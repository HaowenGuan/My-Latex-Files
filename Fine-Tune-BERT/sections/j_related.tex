\section{Related Work}
\label{sec:related}

\paragraph{Pre-training}
Decades of research have shown that unlabeled text can help learn language representations. Word embeddings were first used~\citep{word2vec,glove}, while subsequently contextual word representations were found  more effective~\citep{elmo}. Most recently, research has shifted towards \emph{fine-tuning} methods \citep{openai-gpt,bert,openai-gpt2}, where entire large pre-trained representations are  fine-tuned for end tasks together with a small number of task-specific parameters. While feature-based unsupervised representations have been successfully used in compact models \citep{johnson2015semi, gururangan-etal-2019-variational}, \textit{inter alia}, the pretraining+fine-tuning approach has not been studied in depth for such small models.

\paragraph{Learning compact models}
In this work we built on \emph{model compression}~\citep{model-compression} and its variant \emph{knowledge distillation}~\citep{distillation}. Other related efforts introduced ways to transfer more information from a teacher to a student model, by sharing intermediate layer activations ~\citep{fitnets,yim2017gift,patient_kd}. We experimented with related approaches, but found only slight gains which were dominated by the gains from pre-training and were not complementary. Prior works have also noted the unavailability of in-domain large-scale transfer data and proposed the use of automatically generated pseudo-examples~\citep{model-compression,kimura2018few}. Here we showed that large-scale general domain text can be successfully used for pre-training instead. A separate line of work uses pruning or quantization to derive smaller models ~\citep{han2015deep,Gupta:2015}. Gains from such techniques are expected to be complementary to PD.

\paragraph{Distillation with unsupervised pre-training}
Early efforts to leverage both unsupervised pre-training and distillation provide pre-trained (possibly contextual) word embeddings as inputs to students, rather than pre-training the student stack. For instance, \citet{distillation-reading-comprehension} use ELMo embeddings, while \citep{distillation-classification,tang2019distilling} use context-independent word embeddings. Concurrent work initializes Transformer students from the bottom layers of a 12-layer BERT model \citep{truncated-pretraining,patient_kd,distil_bert}. The latter continues student LM pre-training via distillation from a more expensive LM teacher. For a different purpose of deriving a single model for multiple tasks through distillation, \citet{clark-etal-2019-bam} use a pre-trained student model of the same size as multiple teacher models. However, none of the prior work has analyzed the impact of unsupervised learning for students in relation to the model size and domain of the transfer set.
