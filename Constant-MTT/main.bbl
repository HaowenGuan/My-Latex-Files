\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{bohdal2020flexible}
Ondrej Bohdal, Yongxin Yang, and Timothy Hospedales.
\newblock Flexible dataset distillation: Learn labels instead of images.
\newblock {\em arXiv preprint arXiv:2006.08572}, 2020.

\bibitem{castro2018end}
Francisco~M Castro, Manuel~J Mar{\'\i}n-Jim{\'e}nez, Nicol{\'a}s Guil, Cordelia
  Schmid, and Karteek Alahari.
\newblock End-to-end incremental learning.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 233--248, 2018.

\bibitem{cazenavette2022dataset}
George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei~A Efros, and
  Jun-Yan Zhu.
\newblock Dataset distillation by matching training trajectories.
\newblock {\em arXiv preprint arXiv:2203.11932}, 2022.

\bibitem{cubuk2018autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock {\em arXiv preprint arXiv:1805.09501}, 2018.

\bibitem{cubuk2020randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 702--703, 2020.

\bibitem{dcbench}
Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh.
\newblock Dc-bench: Dataset condensation benchmark.
\newblock In {\em NeurIPS (Dataset and Benchmark)}, 2022.

\bibitem{deng2022remember}
Zhiwei Deng and Olga Russakovsky.
\newblock Remember the past: Distilling datasets into addressable memories for
  neural networks.
\newblock {\em arXiv preprint arXiv:2206.02916}, 2022.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
\newblock {\em Deep learning}, volume~1.
\newblock MIT Press, 2016.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et~al.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2(7), 2015.

\bibitem{pmlr-v162-kim22c}
Jang-Hyun Kim, Jinuk Kim, Seong~Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun
  Jeong, Jung-Woo Ha, and Hyun~Oh Song.
\newblock Dataset condensation via efficient synthetic-data parameterization.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, {\em Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of {\em Proceedings
  of Machine Learning Research}, pages 11102--11118. PMLR, 17--23 Jul 2022.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{lee2022dataset}
Saehyung Lee, Sanghyuk Chun, Sangwon Jung, Sangdoo Yun, and Sungroh Yoon.
\newblock Dataset condensation with contrastive signals.
\newblock {\em arXiv preprint arXiv:2202.02916}, 2022.

\bibitem{nguyen2020dataset}
Timothy Nguyen, Zhourong Chen, and Jaehoon Lee.
\newblock Dataset meta-learning from kernel ridge-regression.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{nguyen2021dataset}
Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee.
\newblock Dataset distillation with infinitely wide convolutional networks.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{radford2015unsupervised}
Alec Radford, Luke Metz, and Soumith Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock {\em arXiv preprint arXiv:1511.06434}, 2015.

\bibitem{rebuffi2017icarl}
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph~H
  Lampert.
\newblock icarl: Incremental classifier and representation learning.
\newblock In {\em Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 2001--2010, 2017.

\bibitem{EdgarRiba2019KorniaAO}
Edgar Riba, Dmytro Mishkin, Daniel Ponsa, Ethan Rublee, and Gary Bradski.
\newblock Kornia: an open source differentiable computer vision library for
  pytorch.
\newblock {\em workshop on applications of computer vision}, 2019.

\bibitem{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International Journal of Computer Vision}, 115(3):211--252,
  2015.

\bibitem{tran2020towards}
Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen, and
  Ngai-Man Cheung.
\newblock Towards good practices for data augmentation in gan training.
\newblock {\em arXiv preprint arXiv:2006.05338}, 2:3, 2020.

\bibitem{ulyanov2016instance}
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
\newblock Instance normalization: The missing ingredient for fast stylization.
\newblock {\em arXiv preprint arXiv:1607.08022}, 2016.

\bibitem{wang2022cafe}
Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, Guan Huang,
  Hakan Bilen, Xinchao Wang, and Yang You.
\newblock Cafe learning to condense dataset by aligning features.
\newblock In {\em IEEE/CVF Conference on Computer Vision and Pattern
  Recognition 2022}, 2022.

\bibitem{wang2021rethinking}
Ruochen Wang, Minhao Cheng, Xiangning Chen, Xiaocheng Tang, and Cho-Jui Hsieh.
\newblock Rethinking architecture selection in differentiable nas.
\newblock In {\em International Conference on Learning Representation}, 2021.

\bibitem{wang2018dataset}
Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei~A Efros.
\newblock Dataset distillation.
\newblock {\em arXiv preprint arXiv:1811.10959}, 2018.

\bibitem{wolf2011facility}
Gert~W Wolf.
\newblock Facility location: concepts, models, algorithms and case studies.
  series: Contributions to management science.
\newblock {\em International Journal of Geographical Information Science},
  25(2):331--333, 2011.

\bibitem{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock {\em arXiv preprint arXiv:1708.07747}, 2017.

\bibitem{xiong2022FedDM}
Yuanhao Xiong, Ruochen Wang, Minhao Cheng, Felix Yu, and Cho-Jui Hsieh.
\newblock Feddm: Iterative distribution matching for communication-efficient
  federated learning, 2022.

\bibitem{zhaodsa}
Bo Zhao and Hakan Bilen.
\newblock Dataset condensation with differentiable siamese augmentation.
\newblock In {\em International Conference on Machine Learning}, pages
  12674--12685. PMLR, 2021.

\bibitem{zhaodm}
Bo Zhao and Hakan Bilen.
\newblock Dataset condensation with distribution matching.
\newblock {\em arXiv preprint arXiv:2110.04181}, 2021.

\bibitem{zhaodc}
Bo Zhao, Konda~Reddy Mopuri, and Hakan Bilen.
\newblock Dataset condensation with gradient matching.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{zhao2020differentiable}
Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han.
\newblock Differentiable augmentation for data-efficient gan training.
\newblock {\em Advances in Neural Information Processing Systems},
  33:7559--7570, 2020.

\bibitem{zhou2022dataset}
Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba.
\newblock Dataset distillation using neural feature regression.
\newblock {\em arXiv preprint arXiv:2206.00719}, 2022.

\bibitem{zhou2020distilled}
Yanlin Zhou, George Pu, Xiyao Ma, Xiaolin Li, and Dapeng Wu.
\newblock Distilled one-shot federated learning.
\newblock {\em arXiv preprint arXiv:2009.07999}, 2020.

\end{thebibliography}
