\documentclass[10pt]{article} % For LaTeX2e

\pdfoutput=1
\PassOptionsToPackage{compress, sort}{natbib}

% \usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{enumitem}
\usepackage{url}            % simple URL typesetting
\usepackage{xspace}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{mathtools}
\usepackage[capitalise, noabbrev]{cleveref}

% https://tex.stackexchange.com/questions/50747/options-for-appearance-of-links-in-hyperref
\hypersetup{
    % hidelinks = true,
    colorlinks = true,
    citecolor = [RGB]{0,130,130},
    urlcolor = [RGB]{200,0,100}
}

\input{general_definitions.tex}

\title{Data Distillation: A Survey}
% \title{Data Distillation Approaches in Machine Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Noveen Sachdeva \email nosachde@ucsd.edu \\
      \addr Computer Science \& Engineering\\
      University of California, San Diego
      \AND
      \name Julian McAuley \email jmcauley@ucsd.edu \\
      \addr Computer Science \& Engineering\\
      University of California, San Diego%
      }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}

\maketitle

\begin{abstract}
    The popularity of deep learning has led to the curation of a vast number of massive and multifarious datasets. Despite having close-to-human performance on individual tasks, training parameter-hungry models on large datasets poses multi-faceted problems such as (a) high model-training time; (b) slow research iteration; and (c) poor eco-sustainability. As an alternative, \emph{data distillation} approaches aim to synthesize terse data summaries, which can serve as effective drop-in replacements of the original dataset for scenarios like model training, inference, architecture search, \etc
    % What we do in this survey -- extend more?
    In this survey, we present a formal framework for data distillation, along with providing a detailed taxonomy of existing approaches. Additionally, we cover data distillation approaches for different data modalities, namely images, graphs, and user-item interactions (recommender systems), while also identifying current challenges and future research directions. % \looseness=-1
\end{abstract}

\section{Introduction}
\begin{loosedefinition} \label{def:data_distillation}
    {\normalfont \textbf{(Data distillation)}} Approaches that aim to synthesize tiny and high-fidelity data summaries which distill the most important knowledge from a given target dataset. Such distilled summaries are optimized to serve as effective drop-in replacements of the original dataset for efficient and accurate data-usage applications like model training, inference, architecture search, etc.
\end{loosedefinition}

% Reference figure 1
The recent ``scale-is-everything'' viewpoint \citep{scaling_1, scaling_2, scaling_3}, argues that training bigger models (\ie, consisting of a higher number of parameters) on bigger datasets, and using larger computational resources is the sole key for advancing the frontier of artificial intelligence.
On the other hand, a well-reasoned, principled solution will arguably be more amenable to scaling, thereby leading to faster progress \citep{data_quality}. Data distillation (Definition \ref{def:data_distillation}), is a task rooted in the latter school of thought. Clearly, the scale viewpoint still holds, in that if we keep increasing the amount of data (albeit now compressed and of higher quality), we will observe an improvement in both upstream and downstream generalization, but at a faster rate.

% re-usability of knowledge
\paragraph{Motivation.} A terse, high-quality data summary has use cases from a variety of standpoints. First and foremost, it leads to a faster model-training procedure. In turn, faster model training equates to (1) compute-cost saving and expedited research iterations, \ie, the investigative procedure of manually experimenting different ideas; 
% (2) compute-cost saving, \ie, reductions in model-training time also leads to reductions in the amount- and time-of compute resources used; 
and (2) improved eco-sustainability, \ie, lowering the amount of compute time directly leads to a lower carbon footprint from running power-hungry accelerated hardware \citep{chasing_carbon}. Additionally, a small data summary democratizes the entire pipeline, as more people can train state-of-the-art algorithms on reasonably accessible hardware using the data summary. Finally, 
a high-quality data summary indirectly also accelerates orthogonal procedures like neural architecture search \citep{darts_nas}, approximate nearest neighbour search \citep{ann}, knowledge distillation \citep{knowledge_distillation}, \etc, where the procedure needs to iterate over the entire dataset multiple times.
% a high-quality data summary also inherently promotes faster knowledge transfer. More specifically, all learning algorithms rely on a collected source of data to estimate their parameters from. Having a high-quality, reusable data summary can be used to train a wide variety of learning algorithms for individual downstream use-cases, without being constrained to share knowledge amongst each other. 

\paragraph{Comparison with knowledge distillation \& transfer learning.} Despite 
%all procedures 
inherently distilling some kind of knowledge, we would like to highlight both \emph{knowledge distillation} and \emph{transfer learning} are orthogonal procedures to data distillation, which can potentially work together to perform both tasks more efficiently. Knowledge distillation \citep{knowledge_distillation} entails distilling the knowledge from a trained teacher network into a smaller student network efficiently. On the other hand, transfer learning \citep{transfer_learning} is the procedure of transferring knowledge across similar tasks, \eg, from image classification to image segmentation. Orthogonally, data distillation aims to distill the knowledge from a given dataset into a terse data summary. Such data summaries can be used \emph{in conjunction} with knowledge distillation or transfer learning procedures for both (1) faster learning of the teacher models; and (2) faster knowledge transfer to the student models. The same comparison holds true for model compression techniques \citep{model_compression} as well, where similar to knowledge distillation, the goal is to reduce model storage size, rather than reducing the training time or sample complexity.

\emph{In this survey}, we intend to provide a succinct overview of various data distillation frameworks across different data modalities. We start by presenting a formal data distillation framework
% , along with discussing commonly employed optimization techniques 
in \cref{sec:framework}, along with a detailed empirical comparison of existing image distillation techniques. Subsequently, in \cref{sec:data_modalities}, we discuss existing data distillation frameworks for synthesizing data of different modalities, as well as outlining the associated challenges. In \cref{sec:applications}, we discuss alternative applications of synthesizing a high-fidelity data summary rather than simply accelerating model training along with pointers to existing work. Finally, in \cref{sec:challenges_future_work}, we conclude by presenting common pitfalls in existing data distillation techniques, along with proposing interesting directions for future work.

\begin{figure*}[t!] \centering
    \centering
    \includegraphics[width=0.7\linewidth]{figures/overview.pdf}
    \vspace{4pt}
    \renewcommand\figurename{\href{https://www.noveens.com/images/dd_survey/overview.pdf}{[HQ Image Link]} Figure}
    \caption{The premise of data distillation demonstrated using an image dataset.}
    \label{fig:overview}
\end{figure*} 

\section{The Data Distillation Framework} \label{sec:framework}

Before going into the specifics of data distillation, we start by outlining useful notation. Let $\dataset \triangleq \{ (x_i, y_i) \}_{i=1}^{|\dataset|}$ be a given dataset which needs to be distilled, where $x_i \in \mathcal{X}$ are the set of input features, and $y_i \in \mathcal{Y}$ is the desired label for $x_i$. For classification tasks, let $\mathcal{C}$ be the set of unique classes in $\mathcal{Y}$, and $\dataset^c \triangleq \{ (x_i, y_i) ~|~ y_i = c\}_{i=1}^{|\dataset|}$ be the subset of $\dataset$ with class $c$. We also define the matrices $\mathbf{X} \triangleq [x_i]_{i=1}^{|\dataset|}$ and $\mathbf{Y} \triangleq [y_i]_{i=1}^{|\dataset|}$ for convenience. Given a data budget $n \in \mathbb{Z}^+$, data distillation techniques aim to synthesize a high-fidelity data summary $\distill \triangleq \{ (\Tilde{x}_i, \Tilde{y}_i) \}_{i=1}^{n}$ such that $n \ll |\dataset|$. We define $\distill^c$, $\mathbf{X}_{\mathsf{syn}}$, and $\mathbf{Y}_{\mathsf{syn}}$ similarly as defined for \dataset. Let $\Phi_{\theta} : \mathcal{X} \mapsto \mathcal{Y}$ represent a learning algorithm parameterized by $\theta$. We also assume access to a twice-differentiable cost function $l : \mathcal{Y} \times \mathcal{Y} \mapsto \mathbb{R}$, and define $\mathcal{L}_{\dataset}(\theta) \triangleq \mathbb{E}_{(x, y) \sim \dataset}[l(\Phi_{\theta}(x), y)]$ for convenience. Notation is also summarized in \cref{appendix:details}.

For the sake of uniformity, we refer to the data synthesized by data distillation techniques as a \emph{data summary} henceforth. Inspired by the definition of coresets \citep{coreset_general}, we formally define an \ $\epsilon-$approximate data summary, and the data distillation task as follows:
\begin{mydefinition} \label{def:data_quality}
    {\normalfont \textbf{(\boldmath{$\epsilon-$}approximate data summary)}} Given a learning algorithm $\Phi$, let $\theta^{\dataset}$, $\theta^{\distill}$ represent the optimal set of parameters for $\Phi$ estimated on $\dataset$ and $\distill$, and $\epsilon \in \mathbb{R}^+$; we define an $\epsilon-$approximate data summary as one which satisfies:
    \begin{equation} \label{eqn:eps_approx_summary}
        \operatorname{sup} ~ \left\{ ~ \left\vert \ l\left(\Phi_{\theta^{\dataset}}(x), y\right) - l\left(\Phi_{\theta^{\distill}}(x), y\right) \ \right\vert ~ \right\}_{\raisebox{6pt}{$\substack{x \sim \mathcal{X}\\y \sim \mathcal{Y}}$}} ~ \leq ~ \epsilon
    \end{equation}
\end{mydefinition}

\begin{mydefinition} \label{def:data_distillation_technical}
    {\normalfont \textbf{(Data distillation)}} Given a learning algorithm $\Phi$, let $\theta^{\dataset}$, $\theta^{\distill}$ represent the optimal set of parameters for $\Phi$ estimated on $\dataset$ and $\distill$; we define data distillation as optimizing the following:
    \begin{equation} \label{eqn:dd_optimization}
        \underset{\distill, n}{\operatorname{arg} \operatorname{min}} \left( \operatorname{sup} ~ \left\{ ~ \left\vert \ l\left(\Phi_{\theta^{\dataset}}(x), y\right) - l\left(\Phi_{\theta^{\distill}}(x), y\right) \ \right\vert ~ \right\}_{\raisebox{6pt}{$\substack{x \sim \mathcal{X}\\y \sim \mathcal{Y}}$}} \right)
    \end{equation}
\end{mydefinition}

% \paragraph{How to evaluate data distillation techniques?} 
From \cref{def:data_distillation_technical}, we highlight three cornerstones of evaluating data distillation methods: (1) Performance: downstream evaluation of models trained on the synthesized data summary \vs the full dataset (\eg, accuracy, FID, nDCG, \etc); (2) Efficiency: how quickly can models reach full-data performance (or even exceed it), \ie, the scaling of $n$ \vs downstream task-performance; and (3) Transferability: how well can data summaries generalize to a diverse pool of learning algorithms, in terms of downstream evaluation.

\paragraph{No free lunch.} The universal ``No Free Lunch'' theorem \citep{no_free_lunch} applies to data distillation as well. 
For example, looking at the transferability of a data summary, it
% which in our scenario translates to the fact that the transferability of a data summary 
is strongly dependent on the set of encoded inductive biases, \ie, through the choice of the learning algorithm $\Phi$ used while distilling, as well as the objective function $l(\cdot, \cdot)$. Such biases are unavoidable for any data distillation technique, 
in a sense that learning algorithms closely following the set of encoded inductive biases, will be able to generalize better on the data summary than others.

Keeping these preliminaries in mind, we now present a formal framework for data distillation, encapsulating existing data distillation approaches. Notably, the majority of existing techniques intrinsically solve a bilevel optimization problem, which are tractable surrogates of \cref{eqn:dd_optimization}. The inner-loop typically optimizes a representative learning algorithm on the data summary, and using the optimized learning algorithm, the outer-loop optimizes a tractable proxy of \cref{eqn:dd_optimization}. 

Some common assumptions that existing data distillation techniques follow are: (1) static-length data summary, \ie, $n$ is fixed and is treated as a tunable hyper-parameter; and (2) we have on-demand access to the target dataset $\dataset$ which is also assumed to be \texttt{iid}. Notably, the outer-loop optimization of \distill happens simply through gradient descent (GD) on the analogously defined $\mathbf{X}_{\mathsf{syn}} \in \mathbb{R}^{n \times \dim(\mathcal{X})}$, which is instantiated as free parameters. Note that the labels, $\mathbf{Y}_{\mathsf{syn}} \in \mathbb{R}^{n \times \dim(\mathcal{Y})}$, can be similarly optimized through GD as well \citep{label_solve}. For the sake of notational clarity, we will interchangeably use optimization of \distill \emph{or} $(\mathbf{X}_{\mathsf{syn}}, \mathbf{Y}_{\mathsf{syn}})$ henceforth.

\begin{figure*}[t!] \centering
    \centering
    \includegraphics[width=0.8\linewidth]{figures/taxonomy.pdf}
    \vspace{4pt}
    \renewcommand\figurename{\href{https://www.noveens.com/images/dd_survey/taxonomy.pdf}{[HQ Image Link]} Figure}
    \caption{A taxonomy of existing data distillation approaches.}
    \label{fig:taxonomy}
\end{figure*} 

\subsection{Data Distillation by Meta-model Matching} \label{sec:meta_model}
Meta-model matching-based data distillation approaches fundamentally optimize for the transferability of models trained on the data summary when generalized to the original dataset:
\begin{equation} \label{eqn:meta_model_matching}
\begin{gathered}
    \underset{\distill}{\operatorname{arg} \operatorname{min}} \hspace{0.4cm} \mathcal{L}_{\dataset}\left(\theta^{\distill}\right) \hspace{0.3cm}
    \text{s.t.} \hspace{0.4cm} \theta^{\distill} ~ \triangleq ~ \underset{\theta}{\operatorname{arg} \operatorname{min}} \hspace{0.15cm} \mathcal{L}_{\distill}(\theta),
\end{gathered}
\end{equation}
where intuitively, the inner-loop trains a representative learning algorithm on the data summary \emph{until convergence}, and the outer-loop subsequently optimizes the data summary for the transferability of the optimized learning algorithm to the original dataset. 
Besides common assumptions mentioned earlier, the key simplifying assumption for this family of methods is that a perfect classifier exists and can be estimated on $\dataset$, \ie, $\exists ~\theta^{\dataset}$ s.t. $l(\Phi_{\theta^{\dataset}}(x), y) = 0, ~ \forall x \sim \mathcal{X}, y \sim \mathcal{Y}$. Plugging the second assumption along with the \texttt{iid} assumption of $\dataset$ in \cref{eqn:dd_optimization} directly translates to \cref{eqn:meta_model_matching}. Despite the assumption, \cref{eqn:meta_model_matching} is highly expensive both in terms of computation time and memory, due to which, methods from this family typically resort to making further assumptions.

\citet{dd_orig} (DD) originally proposed the task of data distillation, and used the meta-model matching framework for optimization. DD makes
% The first works on data distillation used the meta-model matching framework. \citet{dd_orig} (DD) make 
the expensive optimization in \cref{eqn:meta_model_matching} more efficient by performing (1) local optimization \emph{\`a la} stochastic gradient descent (SGD) in the inner-loop, and (2) outer-loop optimization using Truncated Back-Propagation Through Time (TBPTT), \ie, unroll a limited number of inner-loop optimization steps while optimizing the outer-loop. Formally, the modified optimization objective for DD is as follows:
\begin{equation} \label{eqn:dd_orig_optimization}
\begin{gathered}
    \underset{\distill}{\operatorname{arg} \operatorname{min}} \hspace{0.4cm} \expectation{\theta_0 \sim \mathbf{P}_\theta}{\mathcal{L}_{\dataset}\left(\theta_T\right)} \hspace{0.3cm}
    \text{s.t.} \hspace{0.4cm} \theta_{t+1} \leftarrow \theta_t - \eta \cdot \nabla_\theta \mathcal{L}_{\distill}(\theta_t),
\end{gathered}
\end{equation}
where $\mathbf{P}_\theta$ is a parameter initialization distribution of choice, $T$ accounts for the truncation in TBPTT, and $\eta$ is a tunable learning rate. Notably, TBPTT has been associated with drawbacks such as (1) computationally expensive to unroll the inner-loop at each outer-loop update \citep{bilevel_bias_variance}; (2) bias involved with truncated unrolling \citep{biased_bptt}; and (3) poorly conditioned loss landscapes, particularly with long unrolls \citep{bptt_loss_landscape}. Consequently, the TBPTT framework was empirically shown to be ineffective for data distillation in subsequent works \citep{zhao_dc}. However, recent work \citep{remember_past} claims that using momentum-based optimizers and longer unrolling of the inner-loop can greatly improve performance. We delay a deeper discussion of this work to \cref{sec:dd_factorization} for clarity. 
% Missing work here

Analogously, a separate line of work focuses on using Neural Tangent Kernel (NTK) \citep{ntk} based algorithms to solve the inner-loop in closed form. As a brief side note, the infinite-width correspondence states that performing Kernelized Ridge Regression (KRR) using the NTK of a given neural network, is equivalent to training the same $\infty$-width neural network with L2 reconstruction loss for $\infty$ SGD-steps. These ``$\infty$-width'' neural networks have been shown to perform reasonably compared to their finite-width counterparts, while also being solved in closed-form (see \citet{finite_vs_infinite_2} for a detailed analysis on finite \vs infinite neural networks for image classification). KIP uses the NTK of a fully-connected neural network \citep{kip}, or a convolutional network \citep{kip_conv} in the inner-loop of \cref{eqn:meta_model_matching} for efficient data distillation. More formally, given the NTK $\mathcal{K} : \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}$ of a neural network architecture, KIP optimizes the following objective:
\begin{equation} \label{eqn:kip_optimization}
\begin{gathered}
    \underset{\mathbf{X}_{\mathsf{syn}}, \mathbf{Y}_{\mathsf{syn}}}{\operatorname{arg} \operatorname{min}} \hspace{0.4cm} \left\lVert \mathbf{Y} - \mathbf{K}_{\mathbf{X} \mathbf{X}_{\mathsf{syn}}} \cdot (\mathbf{K}_{\mathbf{X}_{\mathsf{syn}} \mathbf{X}_{\mathsf{syn}}} + \lambda I)^{-1} \cdot \mathbf{Y}_{\mathsf{syn}} \right\rVert^2,
\end{gathered}
\end{equation}
where $\mathbf{K}_{AB} \in \mathbb{R}^{|A|\times|B|}$ represents the gramian matrix of two sets $A$ and $B$, and whose $(i, j)^{\text{th}}$ element is defined by $\mathcal{K}(A_i, B_j)$. Although KIP doesn't impose any additional simplifications to the meta-model matching framework, 
it has an $\mathcal{O}(|\dataset| \cdot n \cdot \dim(\mathcal{X}))$ time and memory complexity,
%computing large NTK matrices like $\mathbf{K}_{\mathbf{X}_{\mathsf{syn}} \mathbf{X}_{\mathsf{syn}}}$ for complicated architectures and their inverse are highly compute intensive procedures,
limiting its scalability. Subsequently, RFAD \citep{rfad} proposes using (1) the light-weight Empirical Neural Network Gaussian Process (NNGP) kernel \citep{nngp} instead of the NTK; and (2) a classification loss (\eg, NLL) instead of the L2-reconstruction loss for the outer-loop to get $\mathcal{O}(n)$ time complexity while also having better performance. On a similar note, FRePO \citep{frepo} decouples the feature extractor and a linear classifier in $\Phi$, and alternatively optimizes (1) the data summary along with the classifier, and (2) the feature extractor. To be precise, let $f_{\theta} : \mathcal{X} \mapsto \mathcal{X}'$ be the feature extractor, $g_{\psi} : \mathcal{X}' \mapsto \mathcal{Y}$ be the linear classifier, s.t. $\Phi(x) \equiv g_{\psi}(f_{\theta}(x)) ~\forall x \in \mathcal{X}$; the optimization objective for FRePO can be written as:
\begin{equation} \label{eqn:frepo_optimization}
\begin{gathered}
    \underset{\mathbf{X}_{\mathsf{syn}}, \mathbf{Y}_{\mathsf{syn}}}{\operatorname{arg} \operatorname{min}} \hspace{0.4cm} 
    \expectation{\theta_0 \sim \mathbf{P}_\theta}{\sum_{t=0}^T ~ \left\lVert \mathbf{Y} - \mathbf{K}^{\theta_t}_{\mathbf{X} \mathbf{X}_{\mathsf{syn}}} \cdot (\mathbf{K}^{\theta_t}_{\mathbf{X}_{\mathsf{syn}} \mathbf{X}_{\mathsf{syn}}} + \lambda I)^{-1} \cdot \mathbf{Y}_{\mathsf{syn}} \right\rVert^2} \\
    \text{s.t.} \hspace{0.4cm} \theta_{t+1} \leftarrow \theta_t - \eta \cdot \expectation{(x, y) \sim \distill}{\nabla_\theta l(g_{\psi}(f_{\theta}(x)), y)} ~;~ \mathbf{K}^{\theta}_{\mathbf{X}_{\mathsf{syn}} \mathbf{X}_{\mathsf{syn}}} \triangleq f_{\theta_t}(\mathbf{X}_{\mathsf{syn}}) f_{\theta_t}(\mathbf{X}_{\mathsf{syn}})^T,
\end{gathered}
\end{equation}
where $T$ represents the number of inner-loop update steps for the feature extractor $f_\theta$. Notably, (1) a wide architecture for $f_\theta$ is crucial for distillation quality in FRePO; and (2) despite the bilevel optimization, FRePO is shown to be more scalable compared to KIP (\cref{eqn:kip_optimization}), while also being more generalizable.

\subsection{Data Distillation by Gradient Matching} \label{sec:grad_matching}
Gradient matching based data distillation, at a high level, performs one-step distance matching on (1) the network trained on the target dataset ($\dataset$) \vs (2) the same network trained on the data summary ($\distill$). In contrast to the meta-model matching framework, such an approach circumvents the unrolling of the inner-loop, thereby making the overall optimization much more efficient. 
% is a simplification of the aforementioned meta-model matching family to be more efficient. Approaches in this family aim to match a learning algorithm's gradient on the data summary ($\distill$) \vs on the target dataset ($\dataset$). 
First proposed by \citet{zhao_dc} (DC), data summaries optimized by gradient-matching significantly outperformed heuristic data samplers, principled coreset construction techniques, as well as TBPTT-based data distillation proposed by \citet{dd_orig}. Formally, given a learning algorithm $\Phi$, DC solves the following optimization objective:
\begin{equation} \label{eqn:gradient_matching}
\begin{gathered}
    \underset{\distill}{\operatorname{arg} \operatorname{min}} \hspace{0.4cm} \expectation{\substack{\theta_0 \sim \mathbf{P}_\theta\\c ~\sim~ \mathcal{C}}}{\sum_{t=0}^T ~ \mathbf{D}\left( \nabla_\theta \mathcal{L}_{\dataset^c}(\theta_t), \nabla_\theta \mathcal{L}_{\distill^c}(\theta_t) \right)} \hspace{0.3cm}
    \text{s.t.} \hspace{0.4cm} \theta_{t+1} \leftarrow \theta_t - \eta \cdot \nabla_\theta \mathcal{L}_{\distill}(\theta_t),
\end{gathered}
\end{equation}
where $T$ accounts for model similarity $T$-steps in the future, and $\mathbf{D} : \mathbb{R}^{|\theta|} \times \mathbb{R}^{|\theta|} \mapsto \mathbb{R}$ is a distance metric of choice (typically cosine distance). 
In addition to assumptions imposed by the meta-model matching framework (\cref{sec:meta_model}), gradient-matching assumes (1) inner-loop optimization of only $T$ steps; (2) local smoothness: two sets of model parameters close to each other (given a distance metric) imply model similarity; and (3) first-order approximation of $\theta_t^{\dataset}$: instead of exactly computing the training trajectory of optimizing $\theta_0$ on $\dataset$ (say $\theta_t^{\dataset}$); perform first-order approximation on the optimization trajectory of $\theta_0$ on the much smaller $\distill$ (say $\theta_t^{\distill}$), \ie, approximate $\theta_t^{\dataset}$ as a single gradient-descent update on $\theta_{t-1}^{\distill}$ using $\dataset$ rather than $\theta_{t-1}^{\dataset}$ (\cref{fig:pictorial_gradients}).

% Provide a list of methods with their even subtler variations
Subsequently, numerous other approaches have been built atop this framework with subtle variations. DSA \citep{zhao_dsa} improves over DC by performing the same image-augmentations (\eg, crop, rotate, jitter, \etc) on both \dataset and \distill while optimizing \cref{eqn:gradient_matching}. Since these augmentations are universal and are applicable across data distillation frameworks, augmentations performed by DSA have become a common part of all methods proposed henceforth, but we omit them for notational clarity. DCC \citep{dcc} further modifies the gradient-matching objective to incorporate class contrastive signals inside each gradient-matching step and is shown to improve stability as well as performance. With $\theta_t$ evolving similarly as in \cref{eqn:gradient_matching}, the modified optimization objective for DCC can be written as:
\begin{equation} \label{eqn:dcc_gradient_matching}
\begin{gathered}
    \underset{\distill}{\operatorname{arg} \operatorname{min}} \hspace{0.4cm} \expectation{\theta_0 \sim \mathbf{P}_\theta}{\sum_{t=0}^T ~ \mathbf{D}\left( \expectation{c \in \mathcal{C}}{ \nabla_\theta \mathcal{L}_{\dataset^c}(\theta_t)}, \expectation{c \in \mathcal{C}}{\nabla_\theta \mathcal{L}_{\distill^c}(\theta_t)} \right)}
\end{gathered}
\end{equation}
Most recently, \citet{idc} (IDC) extend the gradient matching framework by: (1) multi-formation: to synthesize a higher amount of data within the same memory budget, store the data summary (\eg, images) in a lower resolution to remove spatial redundancies, and upsample (using \eg, bilinear, FSRCNN \citep{fsrcnn}) to the original scale while usage; and (2) matching gradients of the network's training trajectory over the full dataset $\dataset$ rather than the data summary $\distill$. To be specific, given a $k \times$ upscaling function $f : \mathbb{R}^{d \times d} \mapsto \mathbb{R}^{kd \times kd}$, the modified optimization objective for IDC can be formalized as:
\begin{equation} \label{eqn:idc_gradient_matching}
\begin{gathered}
    \underset{\distill}{\operatorname{arg} \operatorname{min}} \hspace{0.4cm} \expectation{\substack{\theta_0 \sim \mathbf{P}_\theta\\c ~\sim~ \mathcal{C}}}{\sum_{t=0}^T ~ \mathbf{D}\left( \nabla_\theta \mathcal{L}_{\dataset^c}(\theta_t), \nabla_\theta \mathcal{L}_{f(\distill^c)}(\theta_t) \right)} \hspace{0.3cm}
    \text{s.t.} \hspace{0.4cm} \theta_{t+1} \leftarrow \theta_t - \eta \cdot \nabla_\theta \mathcal{L}_{\dataset}(\theta_t)
\end{gathered}
\end{equation}
\citet{idc} further hypothesize that training models on $\distill$ instead of $\dataset$ in the inner-loop has two major drawbacks: (1) strong coupling of the inner- and outer-loop resulting in a chicken-egg problem \citep{em}; and (2) vanishing network gradients due to the small size of $\distill$, leading to an improper outer-loop optimization for gradient-matching based techniques.

\subsection{Data Distillation by Trajectory Matching} \label{sec:traj_matching}
\citet{mtt} proposed MTT which aims to match the training trajectories of models trained on \dataset \vs \distill. More specifically, let $\{ \theta_t^\dataset \}_{t=0}^T$ represent the training trajectory of training $\Phi_\theta$ on \dataset; trajectory matching algorithms aim to solve the following optimization:
\begin{equation} \label{eqn:traj_matching}
\begin{gathered}
    \underset{\distill, \eta}{\operatorname{arg} \operatorname{min}} \hspace{0.4cm} \expectation{\substack{\theta_0 \sim \mathbf{P}_\theta}}{\sum_{t=0}^{T-M} ~ \frac{\mathbf{D}\left( \theta_{t+M}^\dataset, \theta_{t+N}^{\distill} \right)}{\mathbf{D}\left( \theta_{t+M}^\dataset, \theta_{t}^{\dataset} \right)}} \\ % \hspace{0.3cm}
    \text{s.t.} \hspace{0.4cm} \theta_{t+i+1}^{\distill} \leftarrow \theta_{t+i}^{\distill} - \eta \cdot \nabla_\theta \mathcal{L}_{\distill}(\theta_{t+i}^{\distill}) \hspace{0.3cm};\hspace{0.3cm} \theta_{t+1}^{\distill} \leftarrow \theta_{t}^{\dataset} - \eta \cdot \nabla_\theta \mathcal{L}_{\distill}(\theta_{t}^{\dataset}) ,
\end{gathered}
\end{equation}
where $\mathbf{D} : \mathbb{R}^{|\theta|} \times \mathbb{R}^{|\theta|} \mapsto \mathbb{R}$ is a distance metric of choice (typically L2 distance). Such an optimization can intuitively be seen as optimizing for similar quality models trained with $N$ SGD steps on \distill, compared to $M \gg N$ steps on \dataset, thereby invoking long-horizon trajectory matching. Notably, calculating the gradient of \cref{eqn:traj_matching} \wrt \distill encompasses gradient unrolling through $N$-timesteps, thereby limiting the scalability of MTT. On the other hand, since the trajectory of training $\Phi_\theta$ on \dataset, \ie, $\{ \theta_t^\dataset \}_{t=0}^T$ is independent of the optimization of \distill, it can be pre-computed for various $\theta_0 \sim \mathbf{P}_\theta$ initializations and directly substituted. Similar to gradient matching methods (\cref{sec:grad_matching}), the trajectory matching framework also optimizes the first-order distance between parameters, thereby inheriting the local smoothness assumption. As a scalable alternative, \citet{tesla} proposed TESLA, which re-parameterizes the parameter-matching loss of MTT in \cref{eqn:traj_matching} (specifically when $\mathbf{D}$ is set as the L2 distance), using linear algebraic manipulations to make the bilevel optimization's memory complexity independent of $N$. Furthermore, TESLA uses learnable soft-labels ($\mathbf{Y}_{\mathsf{syn}}$) during the optimization for an increased compression efficiency.

\begin{figure*}[t!] \centering
    \centering
    \includegraphics[width=\linewidth]{figures/optimization.pdf}
    \renewcommand\figurename{\href{https://www.noveens.com/images/dd_survey/optimization.pdf}{[HQ Image Link]} Figure}
    \vspace{-10pt}
    \caption{The underlying optimization in various data distillation frameworks.}
    \label{fig:pictorial_gradients}
\end{figure*} 

\subsection{Data Distillation by Distribution Matching} \label{sec:distr_matching}
% \citep{IDC} for theory on why gradient matching is better than distribution matching
Even though the aforementioned gradient-matching or trajectory-matching based data distillation techniques have been empirically shown to synthesize high-quality data summaries, the underlying bilevel optimization, however, is oftentimes an expensive procedure both in terms of computation time and memory. To this end, distribution-matching techniques solve a correlated proxy task which restricts the optimization to a single-level, leading to a much improved scalability. More specifically, instead of matching the quality of models on $\dataset$ \vs $\distill$, distribution-matching techniques directly match the distribution of data in $\dataset$ \vs $\distill$. The key assumption for this family of methods is that two datasets which are similar according to a particular distribution divergence metric, also lead to similarly trained models.

First proposed by \citet{dm}, DM uses (1) numerous parametric encoders to cast high-dimensional data into respective low-dimensional latent spaces; and (2) an approximation of the Maximum Mean Discrepancy to compute the distribution mismatch between $\dataset$ and $\distill$ in each of the latent spaces. More precisely, given a set of $k$ encoders $\mathcal{E} \triangleq \{ \psi_i : \mathcal{X} \mapsto \mathcal{X}_i \}_{i=1}^{k}$, the optimization objective can be written as:
\begin{equation} \label{eqn:distribution_matching}
\begin{gathered}
    \underset{\distill}{\operatorname{arg} \operatorname{min}} \hspace{0.4cm} \expectation{\substack{\psi \sim \mathcal{E}\\c ~\sim~ \mathcal{C}}}{\left\lVert \expectation{x \sim \dataset^c}{\psi(x)} - \expectation{x \sim \distill^c}{\psi(x)} \right\rVert^2}
\end{gathered}
\end{equation}
DM uses a set of randomly initialized neural networks (with the same architecture) to instantiate $\mathcal{E}$. They observe similar performance when instantiated with more meaningful, task-optimized neural networks, despite it being much less efficient. CAFE \citep{cafe} further refines the distribution-matching idea by: (1) solving a bilevel optimization problem for jointly optimizing a \emph{single} encoder ($\Phi$) and the data summary, rather than using a pre-determined \emph{set} of encoders ($\mathcal{E}$); and (2) assuming a neural network encoder ($\Phi$), match the latent representations obtained at all intermediate layers of the encoder instead of only the last layer. Formally, given a $(L+1)$-layer neural network $\Phi_\theta : \mathcal{X} \mapsto \mathcal{Y}$ where $\Phi^l_{\theta}$ represents $\Phi$'s output at the $l^{\text{th}}$ layer, the optimization problem for CAFE can be specified as:
\begin{equation} \label{eqn:cafe_distribution_matching}
\begin{gathered}
    \underset{\distill}{\operatorname{arg} \operatorname{min}} \hspace{0.4cm} \expectation{c ~\sim~ \mathcal{C}}{\sum_{l=1}^{L} \left\lVert \expectation{x \sim \dataset^c}{\Phi^l_{\theta_t}(x)} - \expectation{x \sim \distill^c}{\Phi^l_{\theta_t}(x)} \right\rVert^2  - \beta \cdot \expectation{(x,y)\sim \dataset^c}{\log \hat{p}(y|x, \theta_t)}} \\
    \text{s.t.} \hspace{0.4cm} \theta_{t+1} \leftarrow \theta_t - \eta \cdot \nabla_\theta \mathcal{L}_{\distill}(\theta_t) 
    \hspace{0.2cm} ; \hspace{0.2cm}
    \hat{p}(y|x, \theta) \triangleq \underset{y}{\operatorname{softmax}}\left(\left\langle \Phi^L_\theta(x), \expectation{x' \sim \distill^y}{\Phi^L_\theta(x')} \right\rangle\right),
\end{gathered}
\end{equation}
where $\hat{p}(\cdot|\cdot, \theta)$ intuitively represents the nearest centroid classifier on $\distill$ using the latent representations obtained by last layer of $\Phi_\theta$. Analogously, IT-GAN \citep{gan_distillation} also uses the distribution-matching framework in \cref{eqn:distribution_matching} to generate data that is informative for model training, in contrast to the traditional GAN \citep{gan} which focuses on generating realistic data.

\subsection{Data Distillation by Factorization} \label{sec:dd_factorization}
All of the aforementioned data distillation frameworks intrinsically maintain the synthesized data summary as a large set of free parameters, which are in turn optimized. Arguably, such a setup prohibits knowledge sharing between synthesized data points (parameters), which might introduce data redundancy. On the other hand, factorization-based data distillation techniques parameterize the data summary using two separate components: (1) bases: a set of mutually independent base vectors; and (2) hallucinators: a mapping from the bases' vector space to the joint data- and label-space. In turn, both the bases and hallucinators are optimized for the task of data distillation. 

Formally, let $\mathcal{B} \triangleq \{b_i \in \mathbb{B}\}_{i=1}^{|\mathcal{B}|}$ be the set of bases, and $\mathcal{H} \triangleq \{h_i : \mathbb{B} \mapsto \mathcal{X} \times \mathcal{Y} \}_{i=1}^{|\mathcal{H}|}$ be the set of hallucinators, then the data summary is parameterized as $\distill \triangleq \{ {h(b)} \}_{b\sim \mathcal{B}, ~ h\sim \mathcal{H}}$. Even though such a two-pronged approach seems similar to generative modeling of data, note that unlike classic generative models, (1) the input space consists \emph{only of} a fixed and optimized set of latent codes and isn't meant to take any other inputs; and (2) given a specific $\mathcal{B}$ and $\mathcal{H}$, we can generate at most $|\mathcal{B}|\cdot|\mathcal{H}|$ sized data summaries. Notably, such a hallucinator-bases data parameterization can be optimized using any of the aforementioned data optimization frameworks (\cref{sec:meta_model,sec:grad_matching,sec:distr_matching,sec:traj_matching})

This framework was concurrently proposed by \citet{remember_past} (we take the liberty to term their unnamed model as ``\emph{Lin}-ear \emph{Ba}-ses'') and \citet{haba} (HaBa). LinBa modifies the general hallucinator-bases framework by assuming (1) the bases' vector space ($\mathbb{B}$) to be the same as the task input space ($\mathcal{X}$); and (2) the hallucinator to be linear and additionally conditioned on a given predictand. More specifically, the data parameterization can be formalized as follows:
\begin{equation} \label{eqn:linba_generation}
\begin{gathered}
    \distill \triangleq \left\{ ~ (y \ \mathbf{H}^T\mathbf{B}, ~y) ~ \right\}_{\raisebox{6pt}{$\substack{y \sim \mathcal{C}\\\mathbf{H} \sim \mathcal{H}}$}} \\
    \text{s.t.} \hspace{0.4cm}
    \mathbf{B} \in \mathbb{R}^{|\mathbf{B}| \times \dim(\mathcal{X})} \triangleq \left[ b_i \in \mathcal{X} \right]_{i=1}^{|\mathbf{B}|}
    \hspace{0.3cm} ; \hspace{0.3cm} 
    \mathcal{H} \triangleq \left\{ \mathbf{H}_i \in \mathbb{R}^{|\mathbf{B}| \times |\mathcal{C}|} \right\}_{i=1}^{|\mathcal{H}|},
\end{gathered}
\end{equation}
where for the sake of notational simplicity, we assume $y \in \mathbb{R}^{|\mathcal{C}|}$ represents the one-hot vector of the label for which we want to generate data, and the maximum amount of data that can be synthesized $n \leq |\mathcal{C}|\cdot|\mathcal{H}|$. Since the data generation (\cref{eqn:linba_generation}) is an end-to-end differentiable procedure, both $\mathbf{B}$ and $\mathcal{H}$ are jointly optimized using the TBPTT framework discussed in \cref{sec:meta_model}, albeit with some crucial modifications for vastly improved performance: (1) using momentum-based optimizers instead of vanilla SGD in the inner-loop; and (2) longer unrolling ($\geq 100$ steps) of the inner-loop during TBPTT. \citet{haba} (HaBa) relax the linear and predictand-conditional hallucinator assumption of LinBa, equating to the following data parameterization:
\begin{equation} \label{eqn:haba_generation}
\begin{gathered}
    \distill \triangleq \left\{ ~ (h(b), ~y) ~ \right\}_{\raisebox{6pt}{$\substack{b, y \sim \mathcal{B}\\h \sim \mathcal{H}}$}}
    \hspace{0.3cm} \text{s.t.} \hspace{0.4cm}
    \mathcal{B} \triangleq \left\{ ~ (b_i \in \mathcal{X}, y_i \in \mathcal{Y}) ~ \right\}_{i=1}^{|\mathcal{B}|}
    \hspace{0.3cm} ; \hspace{0.3cm} 
    \mathcal{H} \triangleq \left\{ h_{\theta_i} : \mathcal{X} \mapsto \mathcal{X} \right\}_{i=1}^{|\mathcal{H}|},
\end{gathered}
\end{equation}
where $\mathcal{B}$ and $\mathcal{H}$ are optimized using the trajectory matching framework (\cref{sec:traj_matching}) with an additional contrastive constraint to promote diversity in \distill (cf.~\citet{haba}, Equation (6)). Following this setup, HaBa can generate at most $|\mathcal{B}|\cdot|\mathcal{H}|$ sized data summaries. Furthermore, one striking difference between HaBa (\cref{eqn:haba_generation}) and LinBa (\cref{eqn:linba_generation}) is that to generate each data point, LinBa uses a linear combination of \emph{all} the bases, whereas HaBa generates a data point using a \emph{single} base vector. 

\citet{kfs} (KFS) further build atop this framework by maintaining a different bases' vector space $\mathbb{B}$ from the data domain $\mathcal{X}$, such that $\dim(\mathbb{B}) < \dim(\mathcal{X})$. This parameterization allows KFS to store an even larger number of images, with a comparable storage budget to other methods. Formally, the data parameterization for KFS can be specified as:
\begin{equation} \label{eqn:kfs_generation}
\begin{gathered}
    \distill \triangleq \bigcup_{c \in \mathcal{C}} \left\{ ~ (h(b), ~c) ~ \right\}_{\raisebox{6pt}{$\substack{b \sim \mathcal{B}_c\\h \sim \mathcal{H}}$}} \\
    \text{s.t.} \hspace{0.4cm}
    \mathcal{B} \triangleq \bigcup_{c \in \mathcal{C}} \mathcal{B}_c 
    \hspace{0.3cm} ; \hspace{0.3cm} 
    \mathcal{B}_c \triangleq \left\{ b^c_i \in \mathbb{B} \right\}_{i=1}^{B}
    \hspace{0.3cm} ; \hspace{0.3cm} 
    \mathcal{H} \triangleq \left\{ h_{\theta_i} : \mathbb{B} \mapsto \mathcal{X} \right\}_{i=1}^{|\mathcal{H}|},
\end{gathered}
\end{equation}
where KFS stores $B$ bases per class, equivalent to a total of $n = |\mathcal{C}| \cdot B \cdot |\mathcal{H}|$ sized data summaries. Following this data parameterization, $\mathcal{B}$ and $\mathcal{H}$ are optimized using the distribution matching framework for data distillation (\cref{eqn:distribution_matching}) to ensure fast, single-level optimization.

\paragraph{Data Distillation \vs Data Compression.} We highlight that it is non-trivial to ensure a fair comparison between data distillation techniques that (1) are ``non-factorized'', \ie, maintain each synthesized data point as a set of free-parameters (\cref{sec:meta_model,sec:grad_matching,sec:distr_matching,sec:traj_matching}); and (2) use factorized approaches discussed in this section to efficiently organize the data summary. If we use the size of the data summary ($n$) as the efficiency metric, factorized approaches are adversely affected as they need a much smaller storage budget to synthesize the same-sized data summaries. On the other hand, if we use ``end-to-end bytes of storage'' as the efficiency metric, non-factorized approaches are adversely affected as they perform no kind of data compression, but focus solely on better understanding the model-to-data relationship through the lens of optimization. For a better intuition, one can apply posthoc lossless compression (\eg, Huffman coding) on data synthesized by non-factorized data distillation approaches to fit more images in the same storage budget \citep{less_is_more}. Such techniques 
% More concerningly, in this setup, techniques 
unintentionally deviate from the original intent of data distillation, and progress more toward better data compression techniques. 
%To recommend a solution, 
As a potential solution,
we encourage the community to consider reporting results for both scenarios: a fixed data summary size $n$, as well as fixed bytes-of-storage. Nonetheless, for the ease of empirical comparison amongst the discussed data distillation techniques, we provide a collated set of results over four image-classification datasets in \cref{tab:main_results}.

\input{results.tex}

\begin{figure*}[t!] \centering
    \centering
    \includegraphics[width=0.9\linewidth]{figures/data_modalities.pdf}
    \renewcommand\figurename{\href{https://www.noveens.com/images/dd_survey/data_modalities.pdf}{[HQ Image Link]} Figure}
    % \vspace{-10pt}
    \caption{Overview of distilling data for a few commonly observed data modalities.}
    \label{fig:data_modalities}
\end{figure*}

\section{Data Modalities} \label{sec:data_modalities}
Having learned about different kinds of optimization frameworks for data distillation, we now discuss an orthogonal (and important) aspect of data distillation -- \emph{what kinds of data can data distillation techniques summarize?} From continuous-valued images to heterogeneous, discrete, and semi-structured graphs, the underlying data for each unique application of machine learning has its own modality, structure, and set of assumptions. While the earliest data distillation techniques were designed to summarize images for classification, recent steps have been taken to expand the horizon of data distillation into numerous other scenarios. In what follows, we categorize existing data distillation techniques as per their intended data modality, while also discussing their unique challenges.

\paragraph{Images.} A large-portion of existing data distillation techniques are designed for image classification data \citep{dd_orig, zhao_dc, zhao_dsa, kip, kip_conv, dm, idc, mtt, cafe, gan_distillation, frepo, rfad, dcc, haba, remember_past, kfs} simply because images have a real-valued, continuous data-domain ($\mathcal{X} \equiv \mathbb{R}^{d \times d}$). This allows SGD-based optimization directly on the data, which is treated as a set of free parameters. Intuitively, incrementally changing each pixel value can be treated as slight perturbations in the color space, and hence given a suitable data distillation loss, can be na\"ively optimized using SGD. 

\paragraph{Text.} Textual data is available in large amounts from sources like websites, news articles, academic manuscripts, \etc, and is also readily accessible with datasets like the common crawl\footnote{\href{https://commoncrawl.org/the-data/}{https://commoncrawl.org/the-data/}} which sizes up to almost 541TB. Furthermore, with the advent of large language models (LLM) \citep{bert, gpt, lamda}, training such models from scratch on large datasets has become an increasingly expensive procedure. Despite recent efforts in democratizing LLM training \citep{cramming, huggingface, bloom}, effectively distilling large-scale textual data as a solution is yet to be explored. The key bottlenecks for distilling textual data are: (1) the inherently discrete nature of data, where a token should belong in a limited vocabulary of words; (2) the presence of a rich underlying structure, \ie, sentences of words (text) obey fixed patterns according to a grammar; and (3) richness of context, \ie, a given piece of text could have wildly different semantic interpretations under different contexts.

\citet{text_distill} take a latent-embedding approach to textual data distillation. On a high level, to circumvent the discreteness of the optimization, the authors perform distillation in a continuous embedding space. More specifically, assuming access to a latent space specified by a \emph{fixed} text-encoder, the authors learn continuous \emph{representations} of each word in the distilled text and optimize it using the TBPTT data-distillation framework proposed by \citet{dd_orig} (\cref{eqn:dd_orig_optimization}). Finally, the distilled text representations are decoded by following a simple nearest-neighbor protocol.

\paragraph{Graphs.} A wide variety of data and applications can inherently be modeled as graphs, \eg, user-item interactions \citep{gnn_recsys_survey, eclare, reviews_sigir}, social networks \citep{gnn_social}, autonomous driving \citep{gnn_self_driving, gapformer}, \etc Taking the example of social networks, these user-user graphs in the modern-era easily scale up to the billion-scale \citep{graph_billion}, calling for principled scaling solutions. Graph distillation could trivially solve a majority of the scale challenges, but synthesizing tiny, high-fidelity graphs has the following hurdles: (1) nodes in a graph can be highly abstract, \eg, users, products, text articles, \etc some of which could be discrete, heterogeneous, or even simply numerical IDs; (2) graphs follow a variety of intrinsic patterns (\eg, spatial \citep{gcn}) which need to be retained in the distilled graphs; and (3) quadratic size of the adjacency matrix could be computationally prohibitive even for moderate-sized graphs.

\citet{graph_distill_iclr_22} propose \textsc{GCond} which distills graphs in the inductive node-classification setting, specified by its node-feature matrix $\mathbf{X}$, adjacency matrix $\mathbf{A}$, and node-target matrix $\mathbf{Y}$. \textsc{GCond} distills the given graph by learning a synthetic node-feature matrix $\mathbf{X}_{\mathsf{syn}}$, and using $\mathbf{X}_{\mathsf{syn}}$ to generate $\mathbf{A}_{\mathsf{syn}} \triangleq f_\theta(\mathbf{X}_{\mathsf{syn}})$ which can be realized, \eg, through a parametric similarity function $\operatorname{sim}_\theta(\cdot, \cdot)$ between the features of two nodes, \ie, $\mathbf{A}_{\mathsf{syn}}^{i, j} \triangleq \sigma(\operatorname{sim}_\theta(\mathbf{X}_{\mathsf{syn}}^{i}, \mathbf{X}_{\mathsf{syn}}^{j}))$, where $\sigma(\cdot)$ is the sigmoid function. Finally, both $\mathbf{X}_{\mathsf{syn}}$ and $\theta$ are optimized using the gradient-matching framework proposed by \citet{zhao_dc} (\cref{eqn:gradient_matching}). Another work \citep{graph_distill_arxiv} (GCDM) shares the same framework as \textsc{GCond} but instead uses the distribution matching framework proposed by \citet{dm} (\cref{eqn:distribution_matching}) to optimize $\mathbf{X}_{\mathsf{syn}}$ and $\theta$. Extending to a graph-classification setting, \citet{graph_distill_kdd_22} further propose \textsc{DosCond} with two major changes compared to \textsc{GCond}: (1) instead of parameterizing the adjacency matrix using a similarity function on $\mathbf{X}_{\mathsf{syn}}$, they maintain a free-parameter matrix $\Omega$ with the same size as the adjacency matrix, and sample each $\mathbf{A}_{\mathsf{syn}}^{i, j}$ entry through an independent Bernoulli draw on $\Omega^{i, j}$ as the prior using the reparameterization trick \citep{reparameter}. Such a procedure ensures differentiability as well as discrete matrix synthesis; and (2) $\mathbf{X}_{\mathsf{syn}}$ and $\Omega$ are still optimized using the gradient-matching framework (\cref{eqn:gradient_matching}), albeit with only a single-step, \ie, $T=1$ for improved scalability and without empirically observing a loss in performance.

\paragraph{Recommender Systems.} The amount of online user-feedback data available for training recommender systems is rapidly increasing \citep{data_increasing_recsys}.
% With our ever-increasing online footprint, the amount of logged data of users interacting with smart recommender systems is also rapidly increasing \citep{data_increasing_recsys}. 
Furthermore, typical user-facing recommender systems need to be periodically re-trained \citep{dlrm}, which adds to requirements for smarter data summarization solutions (see \citet{wsdm22} for background on sampling recommender systems data). However, distilling recommender systems data has the following challenges: (1) the data is available in the form of abstract and discrete \texttt{(userID}, \texttt{itemID}, \texttt{relevance)} tuples, which departs from the typical \texttt{(features}, \texttt{label)} setup; (2) the distribution of both user- and item-popularity follows a strong power-law which leads to data scarcity and unstable optimization; and (3) the data inherits a variety of inherent structures, \eg, sequential patterns \citep{svae, sasrec}, user-item graph patterns \citep{gnn_recsys}, item-item co-occurrence patterns \citep{ease}, missing-not-at-randomness \citep{rec_treatments, def_support}, \etc

\citet{inf_ae} propose \textsc{Distill-CF} which distills implicit-feedback recommender systems data, \ie, when the observed user-item relevance is binary (\eg, click or no-click). Such data can be visualized as a binary user-item matrix $\mathbf{R}$ where each row represents a single user, and each column represents an item. On a high-level, \textsc{Distill-CF} synthesizes fake users along with their item-consumption histories, visualized as a synthetic user-item matrix $\mathbf{R}_{\mathsf{syn}}$. Notably, to preserve semantic meaning, the item-space in $\mathbf{R}_{\mathsf{syn}}$ is the same as in $\mathbf{R}$. To alleviate the data discreteness problem, \textsc{Distill-CF} maintains a sampling-prior matrix $\Omega$ which has the same size as $\mathbf{R}_{\mathsf{syn}}$, and can in-turn be used to generate $\mathbf{R}_{\mathsf{syn}}$ using multi-step Gumbel sampling with replacement \citep{gumbel} for each user's prior in $\Omega$ (equivalent to each row). Such a formulation automatically also circumvents the dynamic user- and item-popularity artifact in recommender systems data, which can analogously be controlled by the row- and column-wise entropy of $\Omega$. Finally, $\Omega$ is optimized using the meta-model matching framework proposed by \citet{kip}. Notably, \citet{inf_ae} also propose infinite-width autoencoders which suit the task of item recommendation while also leading to closed-form computation of the inner-loop in the meta-model matching framework (\cref{eqn:kip_optimization}).

\section{Applications} \label{sec:applications}
While the data distillation task was originally designed to accelerate model training, there are numerous other applications of a high-fidelity data summary. Below we briefly discuss a few such promising applications, along with providing pointers to existing works.

\paragraph{Differential Privacy.} Data distillation was recently shown to be a promising solution for differential privacy as defined by \citet{differential_privacy_dwork}. \citet{privacy_free} show that data distillation techniques can perform better than existing state-of-the-art differentially-private data generators \citep{dp_merf, dp_sinkhorn} on both performance and privacy grounds. Notably, the privacy benefits of data distillation techniques are virtually \emph{free}, as none of these methods were optimized for generating differentially-private data. \citet{dd_privacy_clipped} further modify the gradient matching framework (\cref{eqn:gradient_matching}) by clipping and adding white noise to the gradients obtained on the original dataset while optimization. Such a routine was shown to have better sample utility, while also satisfying strict differential privacy guarantees. From a completely application perspective, data distillation has been used to effectively distill sensitive medical data as well \citep{medical_dd_1, medical_dd_2}. \looseness=-1

\paragraph{Neural Architecture Search (NAS).} Automatic searching of neural-network architectures can alleviate the manual effort, as well as lead to better models
% the tedious and heuristic procedure of manually searching neural network architectures 
(see \citet{nas_survey} for a detailed review). Analogous to using model extrapolation, \ie, extrapolating the performance of an under-trained model architecture on the full dataset; data extrapolation, on the other hand, aims to train models on a small, high-fidelity data sample till convergence. Numerous data distillation techniques \citep{zhao_dc, dd_nas} show promise on small NAS test-beds by employing the data extrapolation framework. However, \citet{dc_bench} show that data distillation \emph{does not} perform well when evaluating diverse architectures on bigger NAS test-beds, calling for better rank-preserving data distillation techniques.

\paragraph{Continual Learning.} Never-ending learning (see \citet{continual} for a detailed review) has been frequently associated with catastrophic forgetting \citep{catast_forgetting}, \ie, patterns extracted from old data/tasks are easily forgotten when patterns from new data/tasks are learned. Data distillation has been shown as an effective solution to alleviate catastrophic forgetting, by simply using the distilled data summary in a replay buffer that is continually updated and used in subsequent data/task training \citep{dd_continual_1, dd_continual_2, dd_continual_3}. \citet{remember_past} show further evidence of a simple \emph{compress-then-recall} strategy outperforming existing state-of-the-art continual learning approaches. Notably, \emph{only} the data summary is stored for each task, and a new model is trained (from scratch) using all previous data summaries, for each new incoming task.

\paragraph{Federated Learning.} Federated or collaborative learning (see \citet{federated_survey} for a detailed survey) involves training a learning algorithm in a decentralized fashion. A standard approach to federated learning is to synchronize local parameter updates to a central server, instead of synchronizing the raw data itself \citep{federated_sync_model}. Data distillation, on the other hand, alleviates the need to synchronize large parametric models across clients and servers, by synchronizing tiny synthesized data summaries to the central server instead. Subsequently, the entire training happens only on the central server. Such data distillation-based federated learning methods \citep{federated_distill_1, federated_distill_2, federated_distill_3, federated_distill_4, federated_distill_5, federated_distill_6} are shown to perform better than model-synchronization based federated learning approaches, while also requiring multiple orders lesser client-server communication.

\section{Challenges \& Future Directions} \label{sec:challenges_future_work}

Despite achieving remarkable progress in data-efficient learning, there are numerous framework-based, theoretical, and application-based directions yet to be explored in data distillation. In what follows, we highlight and discuss such directions for the community to further explore, based either on early evidence or our intuition. \looseness=-1

\paragraph{New data modalities \& settings.} Extending on the discussion in \cref{sec:data_modalities}, existing data distillation techniques have largely been restricted to image-classification settings, due to the easy availability of datasets, and amenable data-optimization. However, taking a step back to the broad field of computer vision (see \citet{cv_book} for a thorough background), there are numerous equally important tasks that can benefit from a high-quality data summary. 
% that are riddled with their own set of intricacies. 
For example, increasing the sample efficiency of training image-generation models is both highly important due to their massive size and popularity \citep{dalle, stable_diffusion}, and is also highly non-trivial to fit into the existing data distillation framework. Similarly, a variety of important machine learning applications don't enjoy a continuous data domain like images, making it hard for existing data distillation techniques to scale and work as expected. In addition to recent efforts on distilling discrete data like graphs \citep{graph_distill_kdd_22, graph_distill_iclr_22} and recommender systems \citep{inf_ae}, developing a unified, principled data distillation framework for inherently sparse and discrete data will be useful for a variety of research communities (\eg, text, tabular-data, extreme classification, \etc).

\paragraph{Better scaling.} Existing data distillation techniques validate their prowess \emph{only} in the super low-data regime (typically $1-50$ data points per class). However, \citet{dc_bench} show that as we keep scaling the size of the data summary (larger distilled data), most distillation methods collapse to the random-sampling baseline. While convergent behavior is expected, the distillation performance collapses much more rapidly with larger data summaries. Analogously, for data distillation to practically replace full-data training, deeper investigations of the causes and potential fixes of such scaling artifacts are highly necessary.

\paragraph{Improved optimization.} A unifying thread across data distillation techniques is an underlying bilevel optimization, which is provably NP-hard even in the linear inner-optimization case \citep{bilevel_np_hard}. Notably, bilevel optimization has been successfully applied in a variety of other applications like meta-learning \citep{maml, metasgd}, hyper-parameter optimization \citep{hyperopt_maclaurin, hyperopt_vicol}, neural architecture search \citep{darts_nas}, coreset construction \citep{bilevel_coresets, bilevel_coresets_bayesian}, \etc Despite its success, many theoretical underpinnings are yet to be explored, \eg, the effect of commonly-used singleton solution assumption \citep{singleton_bilevel}, the effect of over-parameterization on bilevel optimization \citep{bilevel_implicit_bias}, connections to statistical influence functions \citep{influence_functions}, the bias-variance tradeoff \citep{bilevel_bias_variance}, \etc Clearly, an overall better understanding of bilevel optimization will directly enable the development of better data distillation techniques.

\section*{Acknowledgments}
We sincerely thank Zhiwei Deng, Bo Zhao, and George Cazenavette for their feedback on early drafts of this survey.

{\small
\bibliography{references}}
\bibliographystyle{tmlr}

% \newpage
\appendix

\input{appendix}

\end{document}
